{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers langchain  chromadb PyPDF2"
      ],
      "metadata": {
        "id": "1WJD2Cpr8C5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 xformers"
      ],
      "metadata": {
        "id": "mb3l6utfcAhm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains import RetrievalQA\n",
        "import transformers\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "G6KsuQfm31lm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
        "\n",
        "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "base_model = LlamaForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map='auto',\n",
        ")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=2048,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "pB-0tgtzFYnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "embed_model = HuggingFaceEmbeddings(model_name=embed_model_id)"
      ],
      "metadata": {
        "id": "-Fe9VPw444Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(pdf_path) :\n",
        "  # location of the pdf file/files.\n",
        "  doc_reader = PdfReader(pdf_path)\n",
        "  # read data from the file and put them into a variable called raw_text\n",
        "  raw_text = ''\n",
        "  for i, page in enumerate(doc_reader.pages):\n",
        "      text = page.extract_text()\n",
        "      if text:\n",
        "          raw_text += text\n",
        "  # Splitting up the text into smaller chunks for indexing\n",
        "  text_splitter = CharacterTextSplitter(\n",
        "          separator = \"\\n\",\n",
        "          chunk_size = 1000,\n",
        "          chunk_overlap  = 200, #striding over the text\n",
        "          length_function = len,\n",
        "      )\n",
        "  texts = text_splitter.split_text(raw_text)\n",
        "  return texts"
      ],
      "metadata": {
        "id": "1yoMDRy722st"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = load_pdf(\"/content/2307.09288.pdf\")"
      ],
      "metadata": {
        "id": "Uw_AB_qP-w1-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "id": "91HLbns2iBaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_texts(texts, embedding=embed_model , persist_directory=\"DB\")"
      ],
      "metadata": {
        "id": "mPiazWMO43Pu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectordb.as_retriever(search_kwargs={\"k\":5}))"
      ],
      "metadata": {
        "id": "pMJc5PlQ4neS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is llama2 ?\"\n",
        "llm_response = rag_pipeline(query)[\"result\"]\n",
        "print(llm_response)"
      ],
      "metadata": {
        "id": "8A1aL8I1AHC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio"
      ],
      "metadata": {
        "id": "sDEQ2UMrBbPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate responses from your chatbot model\n",
        "def chatbot_response(input_text):\n",
        "    # Replace this with your chatbot's logic\n",
        "    # For simplicity, we'll just echo back the input text for now\n",
        "    return rag_pipeline(input_text)[\"result\"]"
      ],
      "metadata": {
        "id": "BrKfR4ILBe7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_response,  # Function to generate responses\n",
        "    inputs=gr.Textbox(text=\"Enter your message here\"),  # Text input for user\n",
        "    outputs=\"text\"  # Display the response as text\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(share=True , debug=True)\n"
      ],
      "metadata": {
        "id": "PQRoPHWVBZp8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}